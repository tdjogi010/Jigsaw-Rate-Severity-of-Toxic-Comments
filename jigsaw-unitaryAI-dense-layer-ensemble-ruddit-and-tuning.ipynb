{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_ranking as tfr\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm \n",
    "# import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants ##\n",
    "test_size_percent = 0.2 # 20% test/val data\n",
    "ruddit_data_path = \"./train/removed_redundant_ruddit_with_text.csv\"\n",
    "val_data_path = \"./train/validation_data.csv\"\n",
    "more_toxic_key = \"more_toxic\"\n",
    "less_toxic_key = \"less_toxic\"\n",
    "score_key = \"score\"\n",
    "\n",
    "dense_dim = 768\n",
    "hidden_dim = dense_dim*3 # 3 models\n",
    "batch_size=32\n",
    "\n",
    "margin = 0.5 # maybe less\n",
    "log_dir = \"./log/unitaryAI-dense-layer-ensemble/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ruddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5710, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruddit_df = pd.read_csv(ruddit_data_path)\n",
    "ruddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5710, 3, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./output/unitaryAI_ruddit_dense_output.npy', 'rb') as f:\n",
    "    ruddit_dense_output = np.load(f)\n",
    "\n",
    "ruddit_dense_output.shape # (examples, models, dense_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5710, 2304)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruddit_combined_dense_output = np.concatenate([ruddit_dense_output[:, 0, :], ruddit_dense_output[:, 1, :], ruddit_dense_output[:, 2, :]], axis=-1)\n",
    "ruddit_combined_dense_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4568, 1142)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_ind, val_ind = train_test_split(list(range(ruddit_combined_dense_output.shape[0])) ,test_size = test_size_percent, random_state = 321)\n",
    "len(tr_ind), len(val_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4568, 2304), (1142, 2304), (4568,), (1142,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruddit_train = ruddit_combined_dense_output[tr_ind, :]\n",
    "ruddit_val = ruddit_combined_dense_output[val_ind, :]\n",
    "\n",
    "y_train = ruddit_df[score_key][tr_ind].values\n",
    "y_val = ruddit_df[score_key][val_ind].values\n",
    "\n",
    "ruddit_train.shape, ruddit_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTOXModel():\n",
    "    input = layers.Input((ruddit_combined_dense_output.shape[-1],))\n",
    "    # print(\"tox_input\", input)\n",
    "    x = layers.Dense(ruddit_combined_dense_output.shape[-1])(input) # same size as input \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(ruddit_combined_dense_output.shape[-1]//2)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    output = layers.Dense(1, activation=\"tanh\")(x) # toxicity score # needed tanh for ruddit # not used in another notebook with valid_data only\n",
    "    # print(\"tox_output\", output)\n",
    "\n",
    "    tox_model = keras.Model(inputs=input, outputs=output)\n",
    "    tox_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss= 'mse',# tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "    tox_model.summary()\n",
    "    return tox_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2304)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2304)              5310720   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2304)              9216      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1152)              2655360   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1152)              4608      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1153      \n",
      "=================================================================\n",
      "Total params: 7,981,057\n",
      "Trainable params: 7,974,145\n",
      "Non-trainable params: 6,912\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 23:24:19.035095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-08 23:24:19.057376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-08 23:24:19.057805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-08 23:24:19.058254: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-08 23:24:19.059226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-08 23:24:19.059533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-08 23:24:19.059850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-08 23:24:19.833856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-08 23:24:19.834240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-08 23:24:19.834253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1594] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-02-08 23:24:19.834519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-08 23:24:19.834586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5420 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "tox_model = getTOXModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 23:24:20.113730: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-08 23:24:20.113762: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-08 23:24:20.113793: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
      "2022-02-08 23:24:20.114289: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 23:24:20.191764: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-08 23:24:20.191915: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
      "2022-02-08 23:24:20.251033: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/143 [..............................] - ETA: 3:04 - loss: 0.4123 - root_mean_squared_error: 0.6421"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 23:24:21.539018: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-02-08 23:24:21.563267: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-08 23:24:21.563305: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11/143 [=>............................] - ETA: 6s - loss: 0.6002 - root_mean_squared_error: 0.7747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 23:24:21.863160: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-02-08 23:24:21.863670: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
      "2022-02-08 23:24:21.877272: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 181 callback api events and 178 activity events. \n",
      "2022-02-08 23:24:21.879550: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-08 23:24:21.886840: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_24_21\n",
      "\n",
      "2022-02-08 23:24:21.891923: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_24_21/DESKTOP-KPOCLK7.trace.json.gz\n",
      "2022-02-08 23:24:21.899320: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_24_21\n",
      "\n",
      "2022-02-08 23:24:21.902049: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_24_21/DESKTOP-KPOCLK7.memory_profile.json.gz\n",
      "2022-02-08 23:24:21.913774: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_24_21\n",
      "Dumped tool data for xplane.pb to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_24_21/DESKTOP-KPOCLK7.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_24_21/DESKTOP-KPOCLK7.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_24_21/DESKTOP-KPOCLK7.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_24_21/DESKTOP-KPOCLK7.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_24_21/DESKTOP-KPOCLK7.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 4s 18ms/step - loss: 0.5108 - root_mean_squared_error: 0.7147 - val_loss: 1.0353 - val_root_mean_squared_error: 1.0175\n",
      "Epoch 2/30\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.1245 - root_mean_squared_error: 0.3528 - val_loss: 0.3209 - val_root_mean_squared_error: 0.5664\n",
      "Epoch 3/30\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.0913 - root_mean_squared_error: 0.3021 - val_loss: 0.0960 - val_root_mean_squared_error: 0.3098\n",
      "Epoch 4/30\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.0703 - root_mean_squared_error: 0.2651 - val_loss: 0.0548 - val_root_mean_squared_error: 0.2340\n",
      "Epoch 5/30\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.0568 - root_mean_squared_error: 0.2384 - val_loss: 0.0798 - val_root_mean_squared_error: 0.2826\n",
      "Epoch 6/30\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.0644 - root_mean_squared_error: 0.2538 - val_loss: 0.0546 - val_root_mean_squared_error: 0.2337\n",
      "Epoch 7/30\n",
      "143/143 [==============================] - 3s 18ms/step - loss: 0.0525 - root_mean_squared_error: 0.2292 - val_loss: 0.0953 - val_root_mean_squared_error: 0.3088\n",
      "Epoch 8/30\n",
      "143/143 [==============================] - 2s 17ms/step - loss: 0.0462 - root_mean_squared_error: 0.2149 - val_loss: 0.0342 - val_root_mean_squared_error: 0.1850\n",
      "Epoch 9/30\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.0438 - root_mean_squared_error: 0.2093 - val_loss: 0.0446 - val_root_mean_squared_error: 0.2111\n",
      "Epoch 10/30\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.0459 - root_mean_squared_error: 0.2141 - val_loss: 0.0453 - val_root_mean_squared_error: 0.2130\n",
      "Epoch 11/30\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.0516 - root_mean_squared_error: 0.2271 - val_loss: 0.0726 - val_root_mean_squared_error: 0.2694\n",
      "Epoch 12/30\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.0412 - root_mean_squared_error: 0.2030 - val_loss: 0.0310 - val_root_mean_squared_error: 0.1762\n",
      "Epoch 13/30\n",
      "143/143 [==============================] - 3s 18ms/step - loss: 0.0560 - root_mean_squared_error: 0.2367 - val_loss: 0.0551 - val_root_mean_squared_error: 0.2346\n",
      "Epoch 14/30\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.0452 - root_mean_squared_error: 0.2127 - val_loss: 0.0361 - val_root_mean_squared_error: 0.1901\n",
      "Epoch 15/30\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.0405 - root_mean_squared_error: 0.2013 - val_loss: 0.0326 - val_root_mean_squared_error: 0.1806\n",
      "Epoch 16/30\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.0395 - root_mean_squared_error: 0.1986 - val_loss: 0.0797 - val_root_mean_squared_error: 0.2823\n",
      "Epoch 17/30\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.0460 - root_mean_squared_error: 0.2145 - val_loss: 0.0502 - val_root_mean_squared_error: 0.2240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e504ab520>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, update_freq=1)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True,)\n",
    "\n",
    "tox_model.fit(ruddit_train, y_train, epochs=n_epochs, validation_data=(ruddit_val, y_val), callbacks=[tensorboard_callback, callback], batch_size=batch_size, shuffle=True)\n",
    "# old one acc: 0.7074061773497177 # acc: 0.7080704085021587 # acc: 0.7092328130189306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tox_model.save(\"./output/unitaryAI-dense-layer-ruddit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df = pd.read_csv(val_data_path)\n",
    "# val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 30108, 3, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./output/unitaryAI_validation_data_dense_output.npy', 'rb') as f:\n",
    "    dense_output = np.load(f)\n",
    "\n",
    "dense_output.shape #  (dataloaders, examples, models, dense_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 30108, 2304)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_output =  np.concatenate([dense_output[:, :, 0, :], dense_output[:, :, 1, :], dense_output[:, :, 2, :]], axis=-1)\n",
    "combined_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24086, 6022)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_ind, val_ind = train_test_split(list(range(combined_output.shape[1])) ,test_size = test_size_percent, random_state = 2343)\n",
    "len(tr_ind), len(val_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross check\n",
    "# print(dense_output[0, 0, 0, :5])\n",
    "# print(dense_output[0, 0, 1, :5])\n",
    "# print(dense_output[0, 0, 2, :5])\n",
    "\n",
    "# print(\"idx 1\")\n",
    "# print(dense_output[1, 0, 0, :5])\n",
    "# print(dense_output[1, 0, 1, :5])\n",
    "# print(dense_output[1, 0, 2, :5])\n",
    "\n",
    "# model_idx = 2\n",
    "# combined_output[0, 0, model_idx*dense_dim: model_idx*dense_dim + 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedEmbeddingGenerator(tf.keras.utils.Sequence): # for validation data\n",
    "    def __init__(self, less_toxic_combined_embeddings, more_toxic_combined_embeddings,  batch_size=batch_size, shuffle=True):\n",
    "        self.less_toxic_combined_embeddings = less_toxic_combined_embeddings\n",
    "        self.more_toxic_combined_embeddings = more_toxic_combined_embeddings\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.indexes = np.arange(len(self.less_toxic_combined_embeddings))\n",
    "        self.on_epoch_end() # shuffle once\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch.\n",
    "        return len(self.less_toxic_combined_embeddings) // self.batch_size + 1 if (len(self.less_toxic_combined_embeddings) % self.batch_size) != 0 else 0\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)\n",
    "\n",
    "    def __getitem__(self, idx): # idx -> index batch\n",
    "        # Retrieves the batch of index.\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        less_toxic_combined_embeddings = tf.convert_to_tensor(self.less_toxic_combined_embeddings[indexes], dtype=tf.float64)\n",
    "        more_toxic_combined_embeddings = tf.convert_to_tensor(self.more_toxic_combined_embeddings[indexes], dtype=tf.float64)\n",
    "        # targets = tf.convert_to_tensor([-1]*len(indexes), dtype=tf.float32) # for pytorch marginranking loss\n",
    "        targets = tf.convert_to_tensor([[0, 1]]*len(indexes), dtype=tf.float32) # for tfr.keras.losses.PairwiseHingeLoss()\n",
    "\n",
    "        return [less_toxic_combined_embeddings, more_toxic_combined_embeddings], targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings_train = CombinedEmbeddingGenerator(combined_output[0, tr_ind, :], combined_output[1, tr_ind, :]) # tr_ind\n",
    "\n",
    "combined_embeddings_val = CombinedEmbeddingGenerator(combined_output[0, val_ind, :], combined_output[1, val_ind, :]) # val_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModels(tox_model):\n",
    "\n",
    "    def loss(margin=1):  # https://keras.io/examples/vision/siamese_contrastive/\n",
    "        # def contrastive_loss(y_true, y_pred):\n",
    "        #     print(y_true, y_pred)\n",
    "        #     square_pred = tf.math.square(y_pred)\n",
    "        #     margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        #     return tf.math.reduce_mean(\n",
    "        #         (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        #     )\n",
    "\n",
    "        # return contrastive_loss\n",
    "        def margin_loss(y_true, diff): # https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html \n",
    "            # tf.print(\"y_true\", y_true, \"diff\", diff)\n",
    "            loss = tf.math.maximum( -y_true * diff + margin, 0)\n",
    "            # tf.print(\"loss\", loss)\n",
    "            return tf.math.reduce_mean(loss)\n",
    "\n",
    "        return margin_loss\n",
    "\n",
    "    less_toxic_input = layers.Input((combined_output.shape[-1],))\n",
    "    more_toxic_input = layers.Input((combined_output.shape[-1],))\n",
    "\n",
    "    tower_1 = tox_model(less_toxic_input)\n",
    "    tower_2 = tox_model(more_toxic_input)\n",
    "    merge_layer = tower_1 - tower_2\n",
    "    siamese = keras.Model(inputs=[less_toxic_input, more_toxic_input], outputs=merge_layer) # try to separate predictions using our embeddings from unitary ai !\n",
    "    siamese.compile(loss=loss(margin=margin), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=[loss(margin=margin)])\n",
    "\n",
    "    # merge_layer = tower_1 - tower_2\n",
    "    # siamese = keras.Model(inputs=[less_toxic_input, more_toxic_input], outputs=merge_layer)\n",
    "    # siamese.compile(loss=tfr.keras.losses.PairwiseHingeLoss(), optimizer=tf.keras.optimizers.Adam(1e-5), )#metrics=[tfr.keras.losses.PairwiseHingeLoss()])\n",
    "    # siamese.summary()\n",
    "    return siamese, tox_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tox_model = tf.keras.models.load_model(\"./output/unitaryAI-dense-layer-ruddit\") ### acc: 0.7074061773497177 without finetuning on validation data!!! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: finetuning on valid data does not help at lot. training loss goes down but validation goes up. Clear overfitting :/ One reason being ruddit and validation data have different rates/standards of toxicity (ie different distribution). So cant really train/fine tune on both :/ ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese, tox_model = getModels(tox_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 23:25:07.623738: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-08 23:25:07.623773: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-08 23:25:08.125266: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-08 23:25:08.125413: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/753 [..............................] - ETA: 1:46 - loss: 0.5154 - margin_loss: 0.5154"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 23:25:08.743626: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-08 23:25:08.743658: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-08 23:25:08.896249: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-02-08 23:25:08.896556: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
      "2022-02-08 23:25:08.909310: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 311 callback api events and 306 activity events. \n",
      "2022-02-08 23:25:08.912953: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-08 23:25:08.920996: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_25_08\n",
      "\n",
      "2022-02-08 23:25:08.927492: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_25_08/DESKTOP-KPOCLK7.trace.json.gz\n",
      "2022-02-08 23:25:08.937175: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_25_08\n",
      "\n",
      "2022-02-08 23:25:08.941073: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_25_08/DESKTOP-KPOCLK7.memory_profile.json.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/753 [..............................] - ETA: 1:15 - loss: 0.5034 - margin_loss: 0.5034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 23:25:08.955166: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_25_08\n",
      "Dumped tool data for xplane.pb to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_25_08/DESKTOP-KPOCLK7.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_25_08/DESKTOP-KPOCLK7.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_25_08/DESKTOP-KPOCLK7.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_25_08/DESKTOP-KPOCLK7.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./log/unitaryAI-dense-layer-ensemble/train/plugins/profile/2022_02_08_23_25_08/DESKTOP-KPOCLK7.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753/753 [==============================] - 20s 26ms/step - loss: 0.3423 - margin_loss: 0.3423 - val_loss: 0.4326 - val_margin_loss: 0.4325\n",
      "Epoch 2/30\n",
      "753/753 [==============================] - 19s 26ms/step - loss: 0.3007 - margin_loss: 0.3008 - val_loss: 0.4610 - val_margin_loss: 0.4612\n",
      "Epoch 3/30\n",
      "753/753 [==============================] - 20s 26ms/step - loss: 0.2938 - margin_loss: 0.2938 - val_loss: 0.4394 - val_margin_loss: 0.4393\n",
      "Epoch 4/30\n",
      "753/753 [==============================] - 20s 26ms/step - loss: 0.2903 - margin_loss: 0.2903 - val_loss: 0.4791 - val_margin_loss: 0.4792\n",
      "Epoch 5/30\n",
      "753/753 [==============================] - 19s 26ms/step - loss: 0.2881 - margin_loss: 0.2881 - val_loss: 0.4847 - val_margin_loss: 0.4850\n",
      "Epoch 6/30\n",
      "753/753 [==============================] - 19s 26ms/step - loss: 0.2880 - margin_loss: 0.2880 - val_loss: 0.4464 - val_margin_loss: 0.4467\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, update_freq=1)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True,)\n",
    "\n",
    "history = siamese.fit(\n",
    "    combined_embeddings_train, # target inside it\n",
    "    validation_data=combined_embeddings_val,\n",
    "    epochs=n_epochs,\n",
    "    callbacks=[tensorboard_callback, callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAcc(preds_less_toxic, preds_more_toxic):\n",
    "    accuracy = np.sum((preds_more_toxic > preds_less_toxic))/preds_more_toxic.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.3249750913317835\n"
     ]
    }
   ],
   "source": [
    "# tox_model.predict(combined_embeddings_val)\n",
    "preds = np.zeros((len(val_ind), 2))\n",
    "lastidx = 0\n",
    "for batch in combined_embeddings_val:\n",
    "    less_tox, more_tox = batch[0][0], batch[0][1]\n",
    "    preds[lastidx: lastidx + len(less_tox), 0] = tox_model.predict(less_tox).squeeze(1)\n",
    "    preds[lastidx: lastidx + len(more_tox), 1] = tox_model.predict(more_tox).squeeze(1)\n",
    "    lastidx += len(less_tox)\n",
    "\n",
    "print(\"acc:\", calculateAcc(preds[:, 0], preds[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tox_model.save(\"./output/unitaryAI-dense-layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGS ##\n",
    "\n",
    "1. v1 version: margin ranking from pytorch and distance as  simple diff. better than all my current models. weird thing : val_loss < train_loss. think valid_loss nad train_loss neeed to go down and can be improved \n",
    "\n",
    "loss: 0.3318 - margin_loss: 0.3318 - val_loss: 0.3283 - val_margin_loss: 0.3281 acc: 0.7163732979076719\n",
    "\n",
    "2. adding batch normalization between hidden dense layer 1 and 2 reduced train_loss 10 times but has worse effect on val_loss 3 times and reduced accuracy\n",
    "\n",
    "loss: 0.0300 - margin_loss: 0.0307 - val_loss: 1.0422 - val_margin_loss: 1.0428 acc: 0.617734971770176\n",
    "\n",
    "3. adding batch normalization after input reduced train_loss 10 times but  has worse effect on val_loss 3 times and reduced accuracy to half!\n",
    "\n",
    "loss: 0.0254 - margin_loss: 0.0254 - val_loss: 0.8329 - val_margin_loss: 0.8331 acc: 0.5059780803719695\n",
    "\n",
    "4. Both batch normalization -> way worse\n",
    "\n",
    "loss: 0.0226 - margin_loss: 0.0226 - val_loss: 2.2827 - val_margin_loss: 2.2818 acc: 0.4858850880106277\n",
    "\n",
    "NOTE: dont apply batch normalization to NLP ig (but good for images and cnn i think)\n",
    "\n",
    "5. changing the 2 hidden dense layer activation from relu to tanh didnt help. A bit better perf. \"\"\"Using tanh now\"\"\"\n",
    "\n",
    "loss: 0.3335 - margin_loss: 0.3335 - val_loss: 0.3306 - val_margin_loss: 0.3303 acc: 0.7178678180006642\n",
    "loss: 0.3331 - margin_loss: 0.3331 - val_loss: 0.3252 - val_margin_loss: 0.3248 acc: 0.7205247426104284\n",
    "\n",
    "6. adding one more dense layer with dense_dim//4 -> didnt help much. Total dense layer: 4\n",
    "\n",
    "loss: 0.3341 - margin_loss: 0.3342 - val_loss: 0.3298 - val_margin_loss: 0.3288 acc: 0.7196944536698772\n",
    "loss: 0.3341 - margin_loss: 0.3342 - val_loss: 0.3260 - val_margin_loss: 0.3250 acc: 0.7123879109930256\n",
    "\n",
    "7. remove one last hidden dense layer. -> didnt help Total dense layer: 2. Using 3 dense layer only\n",
    "loss: 0.3328 - margin_loss: 0.3328 - val_loss: 0.3293 - val_margin_loss: 0.3292 acc: 0.7122218532049153\n",
    "loss: 0.3319 - margin_loss: 0.3319 - val_loss: 0.3302 - val_margin_loss: 0.3293 acc: 0.7062437728329458\n",
    "after 20 epoch loss: 0.3281 - margin_loss: 0.3280 - val_loss: 0.3300 - val_margin_loss: 0.3295 acc: 0.7125539687811359\n",
    "\n",
    "8. Adding layer normalization between dense layer -> didnt help.\n",
    "\n",
    "loss: 0.3337 - margin_loss: 0.3337 - val_loss: 0.3279 - val_margin_loss: 0.3284  acc: 0.7165393556957821\n",
    "\n",
    "9. pytorch marginrank loss -> -1 (ie second input is greater)  (less_toxic, more_toxic)\n",
    "loss: 0.3334 - margin_loss: 0.3333 - val_loss: 0.3283 - val_margin_loss: 0.3282 acc: acc: 0.7155430089671205\n",
    "\n",
    "pytorch marginrank loss -> -1 (ie first input is greater)  (more_toxic, less_toxic, )\n",
    "\n",
    "loss: 0.3331 - margin_loss: 0.3331 - val_loss: 0.3286 - val_margin_loss: 0.3284 acc: 0.7168714712720027\n",
    "\n",
    "Note: custom marginranking loss is correct atleast\n",
    "\n",
    "10. Changed loss from pytorch margin ranking to tfr.keras.losses.PairwiseHingeLoss(). Looks more stable but less strict (less configurable) than the former.\n",
    "\n",
    "loss: 0.3331 - val_loss: 0.3270 acc: 0.7175357024244438\n",
    "after 20 epoch: loss: 0.3284 - val_loss: 0.3270 acc: 0.7186981069412155\n",
    "after 30 epoch: loss: 0.3243 - val_loss: 0.3261 acc: 0.7138824310860179\n",
    "\n",
    "10 epochs is good enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_folds = 5\n",
    "# n_epochs = 10\n",
    "# output_model_path = \"./output/unitaryAI-dense-layer-v2/folds/model_%s_%s\"\n",
    "# kf = KFold(n_splits=k_folds, random_state=111, shuffle=True)\n",
    "# for f, (train_index, test_index) in enumerate(kf.split(list(range(combined_output.shape[1])))):\n",
    "#     combined_embeddings_train = CombinedEmbeddingGenerator(combined_output[0, train_index, :], combined_output[1, train_index, :]) # tr_ind\n",
    "#     combined_embeddings_val = CombinedEmbeddingGenerator(combined_output[0, test_index, :], combined_output[1, test_index, :]) # val_ind\n",
    "\n",
    "    \n",
    "#     siamese, tox_model = getModels()\n",
    "\n",
    "#     history = siamese.fit(\n",
    "#         combined_embeddings_train, # target inside it\n",
    "#         validation_data=combined_embeddings_val,\n",
    "#         epochs=n_epochs,\n",
    "#     )\n",
    "\n",
    "#     preds = np.zeros((len(val_ind), 2))\n",
    "#     lastidx = 0\n",
    "#     for batch in combined_embeddings_val:\n",
    "#         less_tox, more_tox = batch[0][0], batch[0][1]\n",
    "#         preds[lastidx: lastidx + len(less_tox), 0] = tox_model.predict(less_tox).squeeze(1)\n",
    "#         preds[lastidx: lastidx + len(more_tox), 1] = tox_model.predict(more_tox).squeeze(1)\n",
    "#         lastidx += len(less_tox)\n",
    "\n",
    "#     acc = calculateAcc(preds[:, 0], preds[:, 1])\n",
    "#     print(\"f\", f, \"acc:\", acc)\n",
    "#     model_name = output_model_path%(str(acc), \"fold_\"+str(f))\n",
    "#     tox_model.save(model_name)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a39880321af88786c208952b03056ebcfbeddf69d7aecdb8c75f75bacd597582"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('TOX_TF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
