{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext import vocab\n",
    "# from torchtext.data import  \n",
    "from torchtext.legacy import data, datasets\n",
    "from torchtext.legacy.data import BucketIterator, TabularDataset , Dataset\n",
    "from torch.utils.data import Sampler, Subset#, Dataset\n",
    "from typing import Sequence, Optional\n",
    "from torch import nn\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove = vocab.GloVe(name=\"6B\", dim=100) # smaller for ruddit\n",
    "# glove.vectors.size()\n",
    "# del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove.itos[glove.stoi[glove.unk_init]]\n",
    "# glove.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants ##\n",
    "comment_key = \"txt\"\n",
    "target_key = \"score\"\n",
    "train_data_path = \"./train/ruddit_with_text.csv\" #\"./train/clean_ruddit_with_text.csv\"\n",
    "embedding_name = \"glove.6B.100d\"\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "k_folds = 5\n",
    "n_epochs = 7\n",
    "batch_size = 256\n",
    "dropout_rate = 0.5\n",
    "output_model_path = \"./output/bilstm_not_clean_ruddit_only/model_%s_%s\" #\"./output/bilstm_civil_only/model_%s_%s\" # loss, more info\n",
    "vocab_path = \"./output/bilstm_not_clean_ruddit_only/ruddit_vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruddit = pd.read_csv(train_data_path)\n",
    "# ruddit.shape, ruddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_field = data.Field(tokenize='spacy', include_lengths=True, use_vocab=True, batch_first=True, lower=True, tokenizer_language='en_core_web_sm') # some preprocessing before as well\n",
    "text_field = data.Field(tokenize='spacy', include_lengths=True, use_vocab=True, batch_first=True, lower=True, tokenizer_language='en_core_web_sm', )\n",
    "label_field = data.Field(dtype=torch.float32, batch_first=True, sequential=False, use_vocab=False, preprocessing=float)\n",
    "fields = [(None, None), (None, None), ('text', text_field), (None, None), ('label', label_field)]# fields = [('text', text_field), ('label', label_field)] # (None, None)]\n",
    "# text_field.build_vocab(ruddit[comment_key], vectors='glove.6B.100d') # we are fitting on dataset, not using all of glove\n",
    "# text_field.vocab.vectors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5838"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruddit = TabularDataset(\n",
    "    path=train_data_path,\n",
    "    format='csv',\n",
    "    fields=fields,\n",
    "    skip_header=True,\n",
    "    \n",
    ")\n",
    "len(ruddit.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15118, 100])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.build_vocab(ruddit, vectors=embedding_name) # discards embedding which are not used in dataset and aligns the indices\n",
    "text_field.vocab.vectors.size()\n",
    "# text_field.vocab = glove\n",
    "# text_field.vocab.stoi[text_field.pad_token] = torch.zeros(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trains size in k folds 4670.4\n",
      "batch size 256\n",
      "number of batches (in 1 fold training) 18.24375\n"
     ]
    }
   ],
   "source": [
    "ts = len(ruddit.examples) * (k_folds - 1)/ k_folds\n",
    "print(\"trains size in k folds\", ts)\n",
    "print(\"batch size\", batch_size)\n",
    "print(\"number of batches (in 1 fold training)\", ts/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_field.build_vocab(ruddit) # not sure if needed\n",
    "# label_field.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_vocab: vocab, hidden_dim: int, output_dim: int, n_layers: int, # vocab_size: int, embedding_dim: int\n",
    "        bidirectional: bool, dropout: float, pad_idx: Optional[int]):\n",
    "        super().__init__()\n",
    "        vocab_size, embedding_dim = embedding_vocab.vectors.size()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_vocab.vectors, freeze=True, padding_idx=pad_idx) # not training embeddding\n",
    "        # self.embedding_layer =  nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True, # imp\n",
    "                           dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim) # bcos birectional\n",
    "        self.dropout_emb = nn.Dropout(dropout) # not sure if same layer object can be used\n",
    "        self.dropout_fc = nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, text, examples_lengths): # text is already padded\n",
    "        embedded = self.dropout_emb(self.embedding_layer(text))\n",
    "        pack_out = nn.utils.rnn.pack_padded_sequence(embedded, examples_lengths.cpu(), batch_first=True)#.to(device)\n",
    "        out_lstm, (hidden, cell) = self.lstm(embedded) # hidden -> (Dâˆ—num_layers, batch , hidden_dim) # D = 2 if bidirectional=True otherwise 1\n",
    "        h1, h2 = hidden[-2, :, :], hidden[-1, :, :]# -2, -1 is taking last hidden state (twice bcos bidirectional) # so h1,h2 -> (batch, hidden_dim)\n",
    "        x = self.dropout_fc(torch.cat((h1, h2), dim=1)) # concatenate along hidden_dim # x -> (batch, hidden_dim*2)\n",
    "        return self.fc(x) # feel like too many dropouts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. write train and test function \n",
    "# 2. use k fold train using BucketIterator (CHECK padding) and save them\n",
    "def test(test_iter: data.iterator.BucketIterator, model: BiLSTM, criterion: nn.MSELoss):\n",
    "    epoch_loss = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            texts, examples_lengths = batch.text\n",
    "            predictions = model(texts, examples_lengths)\n",
    "            predictions = torch.tanh(predictions)\n",
    "            loss = criterion(predictions.squeeze(1), batch.label)\n",
    "            epoch_loss.append(loss.item())\n",
    "    \n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "\n",
    "def train(train_iter: data.iterator.BucketIterator, val_iter: data.iterator.BucketIterator, model: BiLSTM, criterion: nn.MSELoss, optimizer: torch.optim.Optimizer):\n",
    "    epoch_loss_train = []\n",
    "    epoch_loss_test = []\n",
    "\n",
    "    for i, batch in tqdm(enumerate(train_iter)):\n",
    "        optimizer.zero_grad()\n",
    "        texts, examples_lengths = batch.text\n",
    "        # print(texts)\n",
    "        model.train()\n",
    "        predictions = model(texts, examples_lengths)\n",
    "        # predictions = torch.tanh(predictions) # not for civil\n",
    "        loss = criterion(predictions.squeeze(1), batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_train.append(loss.item())\n",
    "        predictions.detach()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # print(\"train loss\", loss.item())\n",
    "        # print(i, torch.cuda.memory_allocated(device=device), torch.cuda.max_memory_allocated(device=device))\n",
    "        # if val_iter != None and i%10 == 0 and i!=0:\n",
    "        #     epoch_loss_test.append(test(val_iter, model, criterion)) # could eval after certain number of batches/updates\n",
    "        #     print(\"batch index\", i, \"loss_train\", epoch_loss_train[-1], \"loss_test\", epoch_loss_test[-1])\n",
    "            # break\n",
    "        \n",
    "    \n",
    "    if val_iter != None:\n",
    "        epoch_loss_test.append(test(val_iter, model, criterion)) # could eval after certain number of batches/updates\n",
    "        # print(\"loss_train\", epoch_loss_train[-1], \"loss_test\", epoch_loss_test[-1])\n",
    "    \n",
    "    \n",
    "    return np.mean(epoch_loss_train), np.mean(epoch_loss_test) if val_iter != None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(iter(ruddit)), next(iter(ruddit)).text, next(iter(ruddit)).label)\n",
    "# ruddit_train_subset = Subset(ruddit, [0,1,2,3])\n",
    "# ruddit_test_subset = Subset(ruddit, [5,4,7])\n",
    "\n",
    "\n",
    "# ruddit_train_subset, ruddit_test_subset = ruddit.split(split_ratio=0.7)\n",
    "\n",
    "# print(next(iter(ruddit_test_subset)).label)\n",
    "# (train_iter,)= BucketIterator.splits((ruddit,),\n",
    "#                                 sort_key=lambda x: len(x.text),  # sort by s attribute (quote)\n",
    "#                                 sort_within_batch=True,\n",
    "#                                 batch_size=batch_size,\n",
    "#                                 device=device) \n",
    "# train_iter= BucketIterator(ruddit,\n",
    "#                                 sort_key=lambda x: len(x.text),  # sort by s attribute (quote)\n",
    "#                                 sort_within_batch=True,\n",
    "#                                 batch_size=batch_size,\n",
    "                                # device=device) \n",
    "\n",
    "# indices = [5,4,7]\n",
    "# train_iter, test_iter = BucketIterator.splits((Dataset([ruddit.examples[i] for i in indices], fields=fields), Dataset([ruddit.examples[i] for i in indices], fields=fields)),\n",
    "#                                 sort_key=lambda x: len(x.text),  # sort by s attribute (quote)\n",
    "#                                 sort_within_batch=True,\n",
    "#                                 batch_size=batch_size,\n",
    "#                                 device=device)\n",
    "# ruddit_train_subset = Dataset(ruddit_df.loc[[1,2,3]].values, fields=fields)\n",
    "# ruddit_test_subset = Dataset(ruddit_df.loc[[4,6,7]].values, fields=fields)\n",
    "# train_iter,test_iter = BucketIterator.splits((ruddit_train_subset, ruddit_test_subset),\n",
    "#                                 sort_key=lambda x: len(x.text),  # sort by s attribute (quote)\n",
    "#                                 sort_within_batch=True,\n",
    "#                                 batch_size=batch_size,\n",
    "#                                 device=device)\n",
    "# print(len(train_iter))\n",
    "# train_iter.create_batches()\n",
    "# print(train_iter.batches)\n",
    "# print(next(iter(train_iter)))\n",
    "\n",
    "# for batch in train_iter:\n",
    "#     text, batch_len = batch.text\n",
    "#     print(text.get_device())\n",
    "#     print(torch.cuda.memory_allocated(device=device), torch.cuda.max_memory_allocated(device=device))\n",
    "    # print(text)\n",
    "    # break\n",
    "#     print(len(text))\n",
    "    # print(text, batch_len)\n",
    "    # print(\"batch.label\", batch.label)\n",
    "    \n",
    "# for ex in ruddit:\n",
    "#     print(ex.text) # ruddit is a Dataset # it wont convert text to tensor but DataLoader (not checked) and BucketIterator will via text_field\n",
    "#     break\n",
    "\n",
    "# for batch in train_iter.batches:\n",
    "#     print('Batch size: %d\\n'% len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 epoch 0 avg train_loss 0.11404155998637802 test_loss 0.1013382226228714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 epoch 1 avg train_loss 0.10475546886262141 test_loss 0.09432916566729546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 epoch 2 avg train_loss 0.0940287803348742 test_loss 0.09207454100251197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 epoch 3 avg train_loss 0.09135029288498979 test_loss 0.0910108059644699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 epoch 4 avg train_loss 0.09063581141986345 test_loss 0.08881203383207321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 epoch 5 avg train_loss 0.08180172819840281 test_loss 0.07854141741991043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 epoch 6 avg train_loss 0.07949320344548476 test_loss 0.08217549547553063\n",
      "Fold 0 avg train_loss 0.0937295493046592 avg test_loss 0.08975452599780899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 epoch 0 avg train_loss 0.11246485027827714 test_loss 0.10435306131839753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 epoch 1 avg train_loss 0.10213008013210799 test_loss 0.09683376029133797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 epoch 2 avg train_loss 0.09734898942865823 test_loss 0.09822099879384041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 epoch 3 avg train_loss 0.09367733115428373 test_loss 0.08862912729382515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 epoch 4 avg train_loss 0.10402490394680124 test_loss 0.09560944736003876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 epoch 5 avg train_loss 0.09322057036977065 test_loss 0.08987650796771049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 epoch 6 avg train_loss 0.08420576743389431 test_loss 0.08099494650959968\n",
      "Fold 1 avg train_loss 0.09815321324911332 avg test_loss 0.09350254993353573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 epoch 0 avg train_loss 0.11221652968149436 test_loss 0.10163026377558708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 epoch 1 avg train_loss 0.10426381857771623 test_loss 0.09750190973281861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 epoch 2 avg train_loss 0.0986201741585606 test_loss 0.09366491585969924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 epoch 3 avg train_loss 0.09685381443092697 test_loss 0.08584127873182297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 epoch 4 avg train_loss 0.086281694667904 test_loss 0.14199174642562867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 epoch 5 avg train_loss 0.08943756689366542 test_loss 0.082682416588068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 epoch 6 avg train_loss 0.08879519979420461 test_loss 0.09249704629182816\n",
      "Fold 2 avg train_loss 0.09663839974349604 avg test_loss 0.09940136820077895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 epoch 0 avg train_loss 0.10958172733846464 test_loss 0.10939157754182816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 epoch 1 avg train_loss 0.10458877800326598 test_loss 0.11295296996831894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 epoch 2 avg train_loss 0.10587693397936068 test_loss 0.11129003465175628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 epoch 3 avg train_loss 0.09768615094454665 test_loss 0.10330906882882118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 epoch 4 avg train_loss 0.09553959985312663 test_loss 0.10510232597589493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 epoch 5 avg train_loss 0.09157659505542956 test_loss 0.09089349955320358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 epoch 6 avg train_loss 0.08137310551185357 test_loss 0.08438036367297172\n",
      "Fold 3 avg train_loss 0.09803184152657825 avg test_loss 0.10247426288468497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 epoch 0 avg train_loss 0.11527502223065025 test_loss 0.1038052998483181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 epoch 1 avg train_loss 0.10684407071063393 test_loss 0.10476048514246941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 epoch 2 avg train_loss 0.10350463500148371 test_loss 0.10963433906435967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 epoch 3 avg train_loss 0.09288895463472918 test_loss 0.09181951582431794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 epoch 4 avg train_loss 0.08398079970165302 test_loss 0.08659720271825791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 epoch 5 avg train_loss 0.0793655251986102 test_loss 0.0806125432252884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 epoch 6 avg train_loss 0.07719997316598892 test_loss 0.07617800012230873\n",
      "Fold 4 avg train_loss 0.09415128294910703 avg test_loss 0.09334391227790287\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=k_folds, random_state=111, shuffle=True)\n",
    "\n",
    "for f, (train_index, test_index) in enumerate(kf.split(ruddit.examples)): # ruddit is a generator and ruddit.examples is list\n",
    "    train_subset = Dataset([ruddit.examples[i] for i in train_index], fields=fields) # subsets dont work!!\n",
    "    test_subset = Dataset([ruddit.examples[i] for i in test_index], fields=fields)\n",
    "    train_iter, test_iter = BucketIterator.splits((train_subset, test_subset),\n",
    "                                sort_key=lambda x: len(x.text),  # sort by s attribute (quote)\n",
    "                                sort_within_batch=True,\n",
    "                                batch_size=batch_size,\n",
    "                                device=device)\n",
    "                            \n",
    "    train_iter.create_batches()\n",
    "    test_iter.create_batches()\n",
    "    model = BiLSTM(text_field.vocab, hidden_dim, output_dim=1, n_layers=n_layers, bidirectional=True, dropout=dropout_rate, pad_idx=text_field.vocab.stoi[text_field.pad_token])\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "    train_losses, test_losses = [], []\n",
    "    for e in range(n_epochs): # should make it early stopping\n",
    "        train_loss, test_loss = train(train_iter, test_iter, model, criterion, optimizer) # train for one epoch but updates = no. of batches\n",
    "        print(\"Fold\", f, \"epoch\", e, \"avg train_loss\", train_loss, \"test_loss\", test_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    \n",
    "    model_name = output_model_path%(str(test_losses[-1]), \"fold_\"+str(f))# saving after every epoch # can save after certain batches\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print(\"Fold\", f, \"avg train_loss\", np.mean(train_losses), \"avg test_loss\", np.mean(test_losses))\n",
    "    # break # using 4:1 train: test ( ie test is 20%)\n",
    "\n",
    "torch.save(text_field, vocab_path)\n",
    "# k_folds = 2\n",
    "# n_epochs = 1\n",
    "# without val at each batch update took 1.40 min on gpu\n",
    "# with no val took 5 sec on gpu\n",
    "# with val at end of epoch took 7 sec on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "534d5d4d85bce353d4ae4039b868f0afe3da2c6eede4b447eaccc97a1fbb1465"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('TOX': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
