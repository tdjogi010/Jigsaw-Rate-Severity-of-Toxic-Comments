{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import emoji\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "val_data_path = \"./train/validation_data.csv\"\n",
    "more_toxic_key = \"more_toxic\"\n",
    "less_toxic_key = \"less_toxic\"\n",
    "batch_size = 32\n",
    "max_length = 512 # override\n",
    "output_path = \"./output/scores/val_scores_%s_%s.csv\" # model name and more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30108, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_df = pd.read_csv(val_data_path)\n",
    "val_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAcc(preds_less_toxic, preds_more_toxic):\n",
    "    accuracy = np.sum((preds_more_toxic > preds_less_toxic))/preds_more_toxic.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CivilDataGenerator(tf.keras.utils.Sequence): # could optimize more like BucketIterator for padding\n",
    "    def __init__(self, texts, scores, tokenizer, batch_size=batch_size, shuffle=True, include_targets=True, max_length=max_length): # texts -> numpy array\n",
    "        self.texts = texts\n",
    "        self.scores = scores\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.include_targets = include_targets\n",
    "        self.max_length = max_length\n",
    "        # Load our BERT Tokenizer to encode the text.\n",
    "        self.tokenizer =  tokenizer # \n",
    "        self.indexes = np.arange(len(self.texts))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch.\n",
    "        return len(self.texts) // self.batch_size + 1 if (len(self.texts) % self.batch_size) != 0 else 0\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)\n",
    "            \n",
    "    def __getitem__(self, idx): # idx -> index batch\n",
    "        # Retrieves the batch of index.\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        texts = self.texts[indexes]\n",
    "        \n",
    "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
    "        # encoded together and separated by [SEP] token.\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            texts.tolist(), # num\n",
    "            add_special_tokens=True, # not really needed in our case. \n",
    "            max_length=self.max_length, # bert has 512 max length # providing our own\n",
    "            return_attention_mask=True, # need bcos to pad to max length\n",
    "            return_token_type_ids=False, # not needed # needed when u have two sentences\n",
    "            padding='max_length', #pad_to_max_length=True, # needed\n",
    "            return_tensors=\"tf\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        # Convert batch of encoded features to numpy array.\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        \n",
    "        # Set to true if data generator is used for training/validation.\n",
    "        if self.include_targets:\n",
    "            scores = np.array(self.scores[indexes], dtype=\"float32\")\n",
    "            return [input_ids, attention_masks], scores\n",
    "        else:\n",
    "            return [input_ids, attention_masks]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT + 2 BiLSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants \n",
    "load_model_path = \"./output/bert-2-bilstm-fine-tuning/bert/final_model\" # after 2nd epoch\n",
    "max_length = 350\n",
    "bert_path =  \"bert-base-uncased\"\n",
    "model_name = \"bert-2-BiLstm\"\n",
    "more_info = \"2-epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_toxic_data = CivilDataGenerator(\n",
    "    val_data_df[more_toxic_key].values,\n",
    "    None, # no target while inferring\n",
    "    tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    include_targets=False, # added for inference\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "less_toxic_data = CivilDataGenerator(\n",
    "    val_data_df[less_toxic_key].values,\n",
    "    None, # no target while inferring\n",
    "    tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    include_targets=False, # added for inference\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 15:30:15.032914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-03 15:30:15.102771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-03 15:30:15.103136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-03 15:30:15.104573: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-03 15:30:15.105904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-03 15:30:15.106208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-03 15:30:15.106501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-03 15:30:15.971266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-03 15:30:15.971728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-03 15:30:15.971742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1594] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-02-03 15:30:15.972103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-03 15:30:15.972303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5150 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(load_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 15:30:33.671083: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-02-03 15:30:36.299397: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-02-03 15:30:37.690223: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "941/941 [==============================] - 477s 502ms/step\n",
      "941/941 [==============================] - 468s 498ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30108, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_on = [less_toxic_data, more_toxic_data]\n",
    "preds = np.zeros((val_data_df.shape[0], len(predict_on))) # preds\n",
    "with tf.device('/device:GPU:0'):\n",
    "    for i, data_iter in enumerate(predict_on):\n",
    "        preds[:, i] = model.predict(\n",
    "            data_iter,\n",
    "            use_multiprocessing=True, # can only be used when x, y are generators\n",
    "            workers=-1,\n",
    "            verbose=1\n",
    "        ).squeeze(1)\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT + 2 BiLSTM  0.6896505912049954\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT + 2 BiLSTM \", calculateAcc(preds[:, 0], preds[:, 1])) # BERT + 2 BiLSTM  0.6878902617244587 # BERT + 2 BiLSTM + fine tune  0.6896505912049954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({less_toxic_key: preds[:, 0], more_toxic_key: preds[:, 1] }).to_csv(output_path%(model_name, more_info), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT + 1 BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants \n",
    "load_model_path = \"./output/bert-1-bilstm/bert/final_model\" # after 2nd epoch\n",
    "max_length = 350\n",
    "bert_path =  \"bert-base-uncased\"\n",
    "model_name = \"bert-1-BiLstm\"\n",
    "more_info = \"2-epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(load_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "941/941 [==============================] - 391s 415ms/step\n",
      "941/941 [==============================] - 392s 417ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30108, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using the same iter as above\n",
    "predict_on = [less_toxic_data, more_toxic_data]\n",
    "preds = np.zeros((val_data_df.shape[0], len(predict_on))) # preds\n",
    "with tf.device('/device:GPU:0'):\n",
    "    for i, data_iter in enumerate(predict_on):\n",
    "        preds[:, i] = model.predict(\n",
    "            data_iter,\n",
    "            use_multiprocessing=True, # can only be used when x, y are generators\n",
    "            workers=-1,\n",
    "            verbose=1\n",
    "        ).squeeze(1)\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT + 1 BiLSTM  0.6840042513617643\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT + 1 BiLSTM \", calculateAcc(preds[:, 0], preds[:, 1])) # BERT + 1 BiLSTM  0.6840042513617643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({less_toxic_key: preds[:, 0], more_toxic_key: preds[:, 1] }).to_csv(output_path%(model_name, more_info), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnitaryAI Detoxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitaryAI.detoxify import Detoxify\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants \n",
    "model_info_dict = {\n",
    "    'original' : {\n",
    "        \"checkpoint\":\"./unitaryAI/toxic_original-c1212f89.ckpt\", \"huggingface_config_path\": \"./unitaryAI/bert-base-uncased\"\n",
    "    },\n",
    "    'unbiased' : {\n",
    "        \"checkpoint\":\"./unitaryAI/toxic_debiased-c7548aa0.ckpt\", \"huggingface_config_path\":\"./unitaryAI/roberta-base\"\n",
    "    },\n",
    "    \"multilingual\" : {\n",
    "        \"checkpoint\":\"./unitaryAI/multilingual_debiased-0b549669.ckpt\",\"huggingface_config_path\": \"./unitaryAI/xlm-roberta-base\"\n",
    "    }\n",
    "}\n",
    "\n",
    "test_data_path = \"./train/comments_to_score.csv\"\n",
    "val_data_path = \"./train/validation_data.csv\"\n",
    "comment_key = \"text\"\n",
    "comment_id_key = \"comment_id\"\n",
    "batch_size = 32\n",
    "\n",
    "model_names = model_info_dict.keys()\n",
    "more_infos = [\"pretrained_only\"]*len(model_info_dict)\n",
    "dense_dim = 768 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, comments, targets, include_target=True):\n",
    "        self.comments = comments\n",
    "        self.targets = targets\n",
    "        self.include_target = include_target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.comments.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        comment = self.comments[idx]\n",
    "        if self.include_target == True:\n",
    "            return comment, self.targets[idx]\n",
    "        else:\n",
    "            return comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_toxic_dataset = CustomDataset(val_data_df[less_toxic_key].values, None, include_target=False)\n",
    "more_toxic_dataset = CustomDataset(val_data_df[more_toxic_key].values, None, include_target=False)\n",
    "\n",
    "less_toxic_dataloader = DataLoader(less_toxic_dataset, batch_size=batch_size, shuffle=False)\n",
    "more_toxic_dataloader = DataLoader(more_toxic_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification\n",
      "RobertaForSequenceClassification\n",
      "XLMRobertaForSequenceClassification\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i, model_name in enumerate(model_info_dict):\n",
    "    model = Detoxify(model_name, checkpoint=model_info_dict[model_name][\"checkpoint\"], huggingface_config_path=model_info_dict[model_name][\"huggingface_config_path\"])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach().cpu().numpy()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(examples_lenght, model_info_dict, test_dataloader, models):\n",
    "    preds = np.zeros((examples_lenght, len(model_info_dict))) # (examples, model)\n",
    "    dense_output = np.zeros((examples_lenght, len(model_info_dict), dense_dim)) # (examples, model, dense_dim)\n",
    "    for i, model_name in enumerate(model_info_dict):\n",
    "        model = models[i]# Detoxify(model_name, checkpoint=model_info_dict[model_name][\"checkpoint\"], huggingface_config_path=model_info_dict[model_name][\"huggingface_config_path\"])\n",
    "        lastidx=0\n",
    "        if model_name == \"original\":\n",
    "            model.model.bert.pooler.dense.register_forward_hook(get_activation(\"dense\"))\n",
    "        else:\n",
    "            model.model.classifier.dense.register_forward_hook(get_activation(\"dense\"))\n",
    "        \n",
    "        \n",
    "        for texts in tqdm(test_dataloader):\n",
    "            preds_dict = model.predict(texts)\n",
    "            dense_output[lastidx: lastidx+len(texts), i, :] = activation[\"dense\"]\n",
    "            # print(activation[\"dense\"])\n",
    "\n",
    "            # print(preds_dict)\n",
    "            for key in preds_dict:\n",
    "                preds[lastidx: lastidx+len(texts), i]+=preds_dict[key]\n",
    "            \n",
    "            lastidx+=len(texts)\n",
    "    \n",
    "    return preds, dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 941/941 [04:14<00:00,  3.70it/s]\n",
      "100%|██████████| 941/941 [03:58<00:00,  3.95it/s]\n",
      "100%|██████████| 941/941 [04:00<00:00,  3.92it/s]\n",
      "100%|██████████| 941/941 [04:13<00:00,  3.72it/s]\n",
      "100%|██████████| 941/941 [03:56<00:00,  3.97it/s]\n",
      "100%|██████████| 941/941 [04:00<00:00,  3.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2, 30108, 3), (2, 30108, 3, 768))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_on = [less_toxic_dataloader, more_toxic_dataloader]\n",
    "preds = np.zeros((len(predict_on), len(more_toxic_dataset), len(model_info_dict))) # (dataloaders, examples, models)\n",
    "dense_output = np.zeros((len(predict_on), len(more_toxic_dataset), len(model_info_dict), dense_dim)) # (dataloaders, examples, models, dense_dim)\n",
    "for i, m_data_loader in enumerate(predict_on):\n",
    "    preds[i, :, :], dense_output[i, :, :, :]   = getScore(len(more_toxic_dataset), model_info_dict, m_data_loader, models)\n",
    "\n",
    "preds.shape, dense_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/unitaryAI_validation_data_dense_output.npy', 'wb') as f:\n",
    "    np.save(f, dense_output) # (dataloaders, examples, models, dense_dim) # 1 gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original 0.6933373189849874\n",
      "unbiased 0.6997143616314601\n",
      "multilingual 0.6979208183871396\n"
     ]
    }
   ],
   "source": [
    "for i, model_name in enumerate(model_names):\n",
    "    print(model_name, calculateAcc(preds[0, :, i], preds[1, :, i]))\n",
    "\n",
    "#original 0.6933373189849874\n",
    "# unbiased 0.6997143616314601\n",
    "# multilingual 0.6979208183871396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model_name in enumerate(model_names):\n",
    "    pd.DataFrame({less_toxic_key: preds[0, :, i], more_toxic_key: preds[1, :, i] }).to_csv(output_path%(model_name, more_infos[i]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM | ruddit only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "model_name = \"2-bilstm\"\n",
    "more_info = \"ruddit-only\"\n",
    "less_toxic_path = \"./train/less_toxic_bilstm.csv\"\n",
    "more_toxic_path = \"./train/more_toxic_bilstm.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_toxic_preds = pd.read_csv(less_toxic_path)\n",
    "more_toxic_preds = pd.read_csv(more_toxic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_toxic_preds = less_toxic_preds.sort_values(by=[\"comment_id\"])\n",
    "more_toxic_preds = more_toxic_preds.sort_values(by=[\"comment_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       comment_id     score\n",
       " 4941            0  0.534476\n",
       " 17967           1  0.496860\n",
       " 14730           2  0.504504\n",
       " 17176           3  0.518141\n",
       " 3278            4  0.522528\n",
       " ...           ...       ...\n",
       " 19422       30103  0.529898\n",
       " 4279        30104  0.536020\n",
       " 20400       30105  0.518016\n",
       " 26648       30106  0.503946\n",
       " 3179        30107  0.501173\n",
       " \n",
       " [30108 rows x 2 columns],\n",
       "        comment_id     score\n",
       " 10187           0  0.483939\n",
       " 19806           1  0.481741\n",
       " 24447           2  0.533612\n",
       " 1532            3  0.521289\n",
       " 25733           4  0.529794\n",
       " ...           ...       ...\n",
       " 20721       30103  0.507247\n",
       " 26107       30104  0.519268\n",
       " 7781        30105  0.585937\n",
       " 5032        30106  0.593413\n",
       " 9619        30107  0.567748\n",
       " \n",
       " [30108 rows x 2 columns])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_toxic_preds, more_toxic_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 bilstm ruddit only  0.6350139497807892\n"
     ]
    }
   ],
   "source": [
    "print(\"2 bilstm ruddit only \", calculateAcc(less_toxic_preds[\"score\"].values, more_toxic_preds[\"score\"].values)) # 0.6350139497807892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## when saving different list use value # if both columns are series, it will join by index!!! (not expected) \n",
    "pd.DataFrame({less_toxic_key: less_toxic_preds[\"score\"].values, more_toxic_key: more_toxic_preds[\"score\"].values }).to_csv(output_path%(model_name, more_info), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a39880321af88786c208952b03056ebcfbeddf69d7aecdb8c75f75bacd597582"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('TOX_TF': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
