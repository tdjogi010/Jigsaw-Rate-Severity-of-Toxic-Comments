{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:51:33.591917Z","iopub.status.busy":"2022-01-27T07:51:33.591128Z","iopub.status.idle":"2022-01-27T07:51:40.738824Z","shell.execute_reply":"2022-01-27T07:51:40.737920Z","shell.execute_reply.started":"2022-01-27T07:51:33.591808Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf # takes 3 min to load!\n","import transformers\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import re\n","import emoji\n","import gc"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:51:40.740696Z","iopub.status.busy":"2022-01-27T07:51:40.740438Z","iopub.status.idle":"2022-01-27T07:51:40.747260Z","shell.execute_reply":"2022-01-27T07:51:40.746323Z","shell.execute_reply.started":"2022-01-27T07:51:40.740666Z"},"trusted":true},"outputs":[],"source":["## Constants\n","target_key = \"score\"\n","text_key = \"text\" #\"txt\"\n","batch_size = 32\n","bert_path =  \"bert-base-uncased\"\n","train_path = \"../input/clean-civil-data-jigsaw-downsampled/clean_civil.csv\"\n","lstm_hidden_dim = 64 # from 768 (bert) to 64  # the decrease is steep; u may lose info\n","dropout_rate = 0.3\n","output_dim = 1\n","max_length = 350\n","n_epochs = 1\n","checkpoint_filepath = \"./bert/ckpt-loss={loss:.5f}-epoch={epoch}-batch={batch}\"\n","save_after_batches = 5000\n","test_size_percent =  0.05\n","test_path = \"./train/removed_redundant_ruddit_with_text.csv\"\n","val_path = \"./train/validation_data.csv\"\n","load_model_path = \"./output/bert-2-bilstm/bert/final_model\""]},{"cell_type":"markdown","metadata":{},"source":["## COMMENTS TO SCORE EVAL"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:51:40.748800Z","iopub.status.busy":"2022-01-27T07:51:40.748499Z","iopub.status.idle":"2022-01-27T07:51:40.868351Z","shell.execute_reply":"2022-01-27T07:51:40.867731Z","shell.execute_reply.started":"2022-01-27T07:51:40.748771Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(5710, 3)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["comments_to_score_df = pd.read_csv(test_path)\n","comments_to_score_df.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:51:40.870609Z","iopub.status.busy":"2022-01-27T07:51:40.870202Z","iopub.status.idle":"2022-01-27T07:51:40.886263Z","shell.execute_reply":"2022-01-27T07:51:40.885440Z","shell.execute_reply.started":"2022-01-27T07:51:40.870580Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["RE_PATTERNS = {\n","    ' american ':\n","        [\n","            'amerikan'\n","        ],\n","    ' adolf ':\n","        [\n","            'adolf'\n","        ],\n","    ' hitler ':\n","        [\n","            'hitler'\n","        ],\n","    ' fuck':\n","        [\n","            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n","            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n","            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n","            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n","            'feck ', ' fux ', 'f\\*\\*', \n","            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n","        ],\n","    ' ass ':\n","        [\n","            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n","                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n","            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n","        ],\n","    ' ass hole ':\n","        [\n","            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n","        ],\n","    ' bitch ':\n","        [\n","            'b[w]*i[t]*ch', 'b!tch',\n","            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n","            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n","        ],\n","    ' bastard ':\n","        [\n","            'ba[s|z]+t[e|a]+rd'\n","        ],\n","    ' trans gender':\n","        [\n","            'transgender'\n","        ],\n","    ' gay ':\n","        [\n","            'gay'\n","        ],\n","    ' cock ':\n","        [\n","            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n","            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n","        ],\n","    ' dick ':\n","        [\n","            ' dick[^aeiou]', 'deek', 'd i c k'\n","        ],\n","    ' suck ':\n","        [\n","            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n","        ],\n","    ' cunt ':\n","        [\n","            'cunt', 'c u n t'\n","        ],\n","    ' bull shit ':\n","        [\n","            'bullsh\\*t', 'bull\\$hit'\n","        ],\n","    ' homo sex ual':\n","        [\n","            'homosexual'\n","        ],\n","    ' jerk ':\n","        [\n","            'jerk'\n","        ],\n","    ' idiot ':\n","        [\n","            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n","                                                                                      'i d i o t'\n","        ],\n","    ' dumb ':\n","        [\n","            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n","        ],\n","    ' shit ':\n","        [\n","            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n","        ],\n","    ' shit hole ':\n","        [\n","            'shythole'\n","        ],\n","    ' retard ':\n","        [\n","            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n","        ],\n","    ' rape ':\n","        [\n","            ' raped'\n","        ],\n","    ' dumb ass':\n","        [\n","            'dumbass', 'dubass'\n","        ],\n","    ' ass head':\n","        [\n","            'butthead'\n","        ],\n","    ' sex ':\n","        [\n","            'sexy', 's3x', 'sexuality'\n","        ],\n","    ' nigger ':\n","        [\n","            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n","        ],\n","    ' shut the fuck up':\n","        [\n","            'stfu'\n","        ],\n","    ' pussy ':\n","        [\n","            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n","        ],\n","    ' faggot ':\n","        [\n","            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n","            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n","        ],\n","    ' mother fucker':\n","        [\n","            ' motha ', ' motha f', ' mother f', 'motherucker',\n","        ],\n","    ' whore ':\n","        [\n","            'wh\\*\\*\\*', 'w h o r e'\n","        ],\n","}"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:51:40.888347Z","iopub.status.busy":"2022-01-27T07:51:40.888035Z","iopub.status.idle":"2022-01-27T07:51:40.912089Z","shell.execute_reply":"2022-01-27T07:51:40.911374Z","shell.execute_reply.started":"2022-01-27T07:51:40.888307Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["CONTRACTION_MAPPING = {\"ain't\": \"is not\", \"'cause\": \"because\", \"could've\": \"could have\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n","\n","def translateAbuseWords(text, patterns=RE_PATTERNS):\n","    text = re.sub(r\"(.)\\1{2,}\", r'\\1', text)\n","    for target, patterns in patterns.items():\n","        for pat in patterns:\n","            text = re.sub(pat, target, text)\n","    return text\n","\n","\n","def clean_contractions(text, mapping=CONTRACTION_MAPPING):\n","    '''\n","    Expand contractions\n","    '''\n","     \n","    specials = [\"’\", \"‘\", \"´\", \"`\"]\n","    for s in specials:\n","        text = text.replace(s, \"'\")\n","    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n","    return text\n","\n","def social_media_clean(full_line):\n","    full_line = re.sub(r'#([^ ]*)', r'\\1', full_line) # #BanTrump -> BanTrump\n","    full_line = re.sub(r'https?://\\S+|www\\.\\S+', ' ', full_line) # URL -> \" \"\n","    full_line = re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9_]+)', ' ', full_line) # @user -> \" \"\n","    full_line = emoji.demojize(full_line) # emoji -> text\n","    full_line = re.sub(r'(:.*?:)', r' \\1 ', full_line) # :emoji-desc: -> emoji-desc\n","    full_line = re.sub(' +', ' ', full_line) #  extra blank spaces have been replaced with a single space.\n","\n","    # repattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n","    # ds = ds.str.replace(repattern, r\"\\1\")\n","    return full_line\n","\n","\n","def preprocess(full_line):\n","    full_line = full_line.lower()\n","    full_line = clean_contractions(full_line)\n","    full_line = translateAbuseWords(full_line)\n","    full_line = social_media_clean(full_line)\n","    full_line = re.sub(r\"[^a-zA-Z\\d]\", \" \", full_line) # messes with emoji\n","    return full_line"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:51:40.913581Z","iopub.status.busy":"2022-01-27T07:51:40.913033Z","iopub.status.idle":"2022-01-27T07:52:06.675435Z","shell.execute_reply":"2022-01-27T07:52:06.674835Z","shell.execute_reply.started":"2022-01-27T07:51:40.913545Z"},"trusted":true},"outputs":[],"source":["# comments_to_score_df[text_key] = comments_to_score_df[text_key].apply(lambda x: preprocess(x)) # PREPROCESS"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:52:06.676813Z","iopub.status.busy":"2022-01-27T07:52:06.676525Z","iopub.status.idle":"2022-01-27T07:52:07.869491Z","shell.execute_reply":"2022-01-27T07:52:07.868672Z","shell.execute_reply.started":"2022-01-27T07:52:06.676782Z"},"trusted":true},"outputs":[],"source":["class CivilDataGenerator(tf.keras.utils.Sequence): # could optimize more like BucketIterator for padding\n","    def __init__(self, texts, scores, tokenizer, batch_size=batch_size, shuffle=True, include_targets=True): # texts -> numpy array\n","        self.texts = texts\n","        self.scores = scores\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.include_targets = include_targets\n","        # Load our BERT Tokenizer to encode the text.\n","        self.tokenizer =  tokenizer # \n","        self.indexes = np.arange(len(self.texts))\n","        self.on_epoch_end()\n","        \n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.texts) // self.batch_size + 1 if (len(self.texts) % self.batch_size) != 0 else 0\n","    \n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.RandomState(42).shuffle(self.indexes)\n","            \n","    def __getitem__(self, idx): # idx -> index batch\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        texts = self.texts[indexes]\n","        \n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n","        # encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            texts.tolist(), # num\n","            add_special_tokens=True, # not really needed in our case. \n","            max_length=max_length, # bert has 512 max length # providing our own\n","            return_attention_mask=True, # need bcos to pad to max length\n","            return_token_type_ids=False, # not needed # needed when u have two sentences\n","            padding='max_length', #pad_to_max_length=True, # needed\n","            return_tensors=\"tf\",\n","            truncation=True,\n","        )\n","        \n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        \n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            scores = np.array(self.scores[indexes], dtype=\"float32\")\n","            return [input_ids, attention_masks], scores\n","        else:\n","            return [input_ids, attention_masks]\n","        "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:52:07.871237Z","iopub.status.busy":"2022-01-27T07:52:07.870902Z","iopub.status.idle":"2022-01-27T07:52:22.847339Z","shell.execute_reply":"2022-01-27T07:52:22.846353Z","shell.execute_reply.started":"2022-01-27T07:52:07.871204Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-01-27 19:04:15.718113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-01-27 19:04:15.782073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-01-27 19:04:15.782479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-01-27 19:04:15.783464: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-01-27 19:04:15.784754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-01-27 19:04:15.785079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-01-27 19:04:15.785356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-01-27 19:04:16.722229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-01-27 19:04:16.722754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-01-27 19:04:16.722766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1594] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n","2022-01-27 19:04:16.723056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-01-27 19:04:16.723187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1638 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n","2022-01-27 19:04:19.420038: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_ids (InputLayer)          [(None, 350)]        0                                            \n","__________________________________________________________________________________________________\n","attention_masks (InputLayer)    [(None, 350)]        0                                            \n","__________________________________________________________________________________________________\n","bert (TFBertMainLayer)          TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n","                                                                 attention_masks[0][0]            \n","__________________________________________________________________________________________________\n","bidirectional (Bidirectional)   (None, 350, 128)     426496      bert[0][0]                       \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 350, 128)     98816       bidirectional[0][0]              \n","__________________________________________________________________________________________________\n","global_average_pooling1d (Globa (None, 128)          0           bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n","                                                                 global_max_pooling1d[0][0]       \n","__________________________________________________________________________________________________\n","dropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 1)            257         dropout_37[0][0]                 \n","==================================================================================================\n","Total params: 110,007,809\n","Trainable params: 525,569\n","Non-trainable params: 109,482,240\n","__________________________________________________________________________________________________\n"]}],"source":["\n","# # Encoded token ids from BERT tokenizer.\n","# input_ids = tf.keras.layers.Input(\n","#     shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n","# )\n","# # Attention masks indicates to the model which tokens should be attended to.\n","# attention_masks = tf.keras.layers.Input(\n","#     shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n","# )\n","\n","# # Loading pretrained BERT model.\n","# bert_model = transformers.TFBertModel.from_pretrained(bert_path)\n","# # Freeze the BERT model to reuse the pretrained features without modifying them.\n","# bert_model.trainable = False ## not training bert ##\n","\n","# bert_output = bert_model.bert(input_ids, attention_mask=attention_masks) # by default hidden_size = 768\n","# sequence_output = bert_output.last_hidden_state  # (batch_size, sequence_length, hidden_size) # ie each word representation \n","\n","# ## for the warning we are good. not using pooled_output for now, https://github.com/huggingface/transformers/issues/5421\n","# # pooled_output = bert_output.pooler_output # (batch_size, hidden_size) # ie whole text representational (kinda)\n","\n","# # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n","# bi_lstm = tf.keras.layers.Bidirectional(\n","#     tf.keras.layers.LSTM(lstm_hidden_dim, return_sequences=True) \n","# )(sequence_output) # (batch_size,  sequence_length, lstm_hidden_dim*2) # merge_mode=\"concat\"\n","\n","# bi_lstm = tf.keras.layers.Bidirectional(\n","#     tf.keras.layers.LSTM(lstm_hidden_dim, return_sequences=True)\n","# )(bi_lstm) # (batch_size,  sequence_length, lstm_hidden_dim*2) # stacked one more BiLSTM bcos with one stack its converging slowly (kind of plateau)\n","\n","# # Applying hybrid pooling approach to bi_lstm sequence output.\n","# avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm) # averages over sequence length # (batch_size, lstm_hidden_dim*2)\n","# max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm) # (batch_size, lstm_hidden_dim*2)\n","# concat = tf.keras.layers.concatenate([avg_pool, max_pool]) #(batch_size, lstm_hidden_dim*3)\n","# dropout = tf.keras.layers.Dropout(dropout_rate)(concat) #(batch_size, lstm_hidden_dim*3)\n","\n","# output = tf.keras.layers.Dense(output_dim)(dropout) # 1 since our target is 1 bcos regression \n","\n","\n","# model = tf.keras.models.Model(\n","#     inputs=[input_ids, attention_masks], outputs=output\n","# )\n","# model.compile(\n","#     optimizer=tf.keras.optimizers.Adam(),\n","#     loss= 'mse',# tf.keras.losses.MeanSquaredError(),\n","#     metrics=[tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.RootMeanSquaredError()],\n","# )\n","\n","# model.summary()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:52:22.850784Z","iopub.status.busy":"2022-01-27T07:52:22.849118Z","iopub.status.idle":"2022-01-27T07:52:27.347557Z","shell.execute_reply":"2022-01-27T07:52:27.346759Z","shell.execute_reply.started":"2022-01-27T07:52:22.850746Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-02-03 21:28:57.097865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-02-03 21:28:57.157336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-02-03 21:28:57.157707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-02-03 21:28:57.158919: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-02-03 21:28:57.159853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-02-03 21:28:57.160100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-02-03 21:28:57.160330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-02-03 21:28:58.269550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-02-03 21:28:58.269957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-02-03 21:28:58.269967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1594] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n","2022-02-03 21:28:58.270303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2022-02-03 21:28:58.270372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5420 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"]}],"source":["# model.load_weights(load_model_path)\n","model = tf.keras.models.load_model(load_model_path)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:52:27.350872Z","iopub.status.busy":"2022-01-27T07:52:27.350471Z","iopub.status.idle":"2022-01-27T07:52:27.415698Z","shell.execute_reply":"2022-01-27T07:52:27.415056Z","shell.execute_reply.started":"2022-01-27T07:52:27.350827Z"},"trusted":true},"outputs":[],"source":["tokenizer = transformers.BertTokenizer.from_pretrained(bert_path, do_lower_case=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:52:27.417448Z","iopub.status.busy":"2022-01-27T07:52:27.416979Z","iopub.status.idle":"2022-01-27T07:52:27.423276Z","shell.execute_reply":"2022-01-27T07:52:27.422261Z","shell.execute_reply.started":"2022-01-27T07:52:27.417404Z"},"trusted":true},"outputs":[],"source":["test_data = CivilDataGenerator(\n","    comments_to_score_df[text_key].values,\n","    None, # no target while inferring\n","    tokenizer,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    include_targets=False # added for inference\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T07:52:27.425161Z","iopub.status.busy":"2022-01-27T07:52:27.424857Z","iopub.status.idle":"2022-01-27T07:53:22.152227Z","shell.execute_reply":"2022-01-27T07:53:22.151131Z","shell.execute_reply.started":"2022-01-27T07:52:27.425121Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-02-03 21:29:19.754690: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","2022-02-03 21:29:22.331327: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","2022-02-03 21:29:26.040137: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n"]},{"name":"stdout","output_type":"stream","text":["179/179 [==============================] - 95s 495ms/step\n"]},{"data":{"text/plain":["(5710, 1)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# preds = np.zeros((comments_to_score_df.shape[0],1))\n","with tf.device('/device:GPU:0'):\n","    preds = model.predict(\n","        test_data,\n","        use_multiprocessing=True, # can only be used when x, y are generators\n","        workers=-1,\n","        verbose=1,\n","    )\n","\n","preds.shape"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.status.busy":"2022-01-27T07:53:22.155056Z","iopub.status.idle":"2022-01-27T07:53:22.155696Z","shell.execute_reply":"2022-01-27T07:53:22.155473Z","shell.execute_reply.started":"2022-01-27T07:53:22.155445Z"},"trusted":true},"outputs":[],"source":["comments_to_score_df[\"score\"] = preds # negatives as well\n","# comments_to_score_df.head()\n","comments_to_score_df[[\"comment_id\", \"score\"]].to_csv(\"./output/bert-2-bilstm-epoch-2-ruddit-preds.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":4}
